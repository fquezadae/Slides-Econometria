---
title: "Modelo de Variable Dependiente Limitada"
subtitle: "Modelos de Sesgo de Selecci贸n"
author: "Felipe Quezada"
date: " "
output: 
  xaringan::moon_reader:
    self_contained: true
    css: ["metropolis", "default-fonts", "udeconce.css"]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    includes:
      in_header: katex.html
---
  
```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r refmanager, include=FALSE}
library(RefManageR)
source(here::here("helper.R"))
BibOptions(check.entries = FALSE, 
           bib.style = "authoryear", 
           style = "markdown",
           dashed = TRUE)
bib <- ReadBib("bibliography.bib")
citeopt <- list(max.names=3, longnamesfirst = FALSE)
```

```{r use-logo, echo=FALSE}
xaringanExtra::use_logo("https://fquezadae.github.io/Slides-Econometria/figs/depto_economia_blanco.png")
```

---

# Censura, Truncamiento y Modelos de Regresi贸n

## Medias Condicionales

Media Condicional **<u>para Censura</u>** por Debajo (umbral $L = 0$):

$$ E(y | d = 0) = E(y^* | y^* \leq 0) = E(y^*) \cdot P(y^* \leq 0) + E(y^* | y^* > 0). $$

- **Interpretaci贸n:** La media condicional de  $y$  dado que est谩 censurado  ($d=0$)  se puede descomponer en dos t茅rminos:
  - La media de  $y^*$  (la variable latente) multiplicada por la probabilidad de que  $y^*$  sea menor o igual a cero (la probabilidad de censura).
  - La media condicional de  $y^*$  dado que  $y^*$  es mayor que cero (la media de las observaciones no censuradas).
  

---

# Censura, Truncamiento y Modelos de Regresi贸n

## Medias Condicionales

Media Condicional **<u>para truncamiento</u>.** La media condicional de  $y$  dado  $x$  y que  $y \geq L$  (truncamiento por debajo) es

$$E(y | x, y \geq L) = E(y^* | x, y^* \geq L) = x_i'\beta + \sigma\lambda,$$
    
donde $\lambda = \frac{\phi(\alpha)}{1 - \Phi(\alpha)}$  es la relaci贸n de Mills inversa.
    
--- 
      
# Censura, Truncamiento y Modelos de Regresi贸n
      
## F贸rmulas bajo supuestos de normalidad
      
**Motivaci贸n:**  Cuando asumimos que los errores en el modelo de regresi贸n censurada o truncada siguen una distribuci贸n normal, podemos derivar expresiones expl铆citas para las medias condicionales.  Estas expresiones son 煤tiles para comprender el impacto de la censura o el truncamiento en las estimaciones y para desarrollar m茅todos de correcci贸n.
    
--- 
      
# Censura, Truncamiento y Modelos de Regresi贸n
      
## Censura
      
Para el modelo de censura por debajo con umbral  $L = 0$, la media condicional de  $\epsilon_i$  dado que  $x_i'\beta + \epsilon_i > 0$  es:

$$E(\epsilon_i | x_i'\beta + \epsilon_i > 0) = \sigma \cdot \lambda(\alpha),$$
      
donde $\alpha = \frac{x_i'\beta}{\sigma}$  es el valor estandarizado de  $x_i'\beta$ y $\lambda(\alpha) = \frac{\phi(\alpha)}{\Phi(\alpha)}$  es la **relaci贸n de Mills inversa**.
      
**Media Condicional de  $y_i$  (Censura):**  La media condicional de  $y_i$  dado  $x_i$  y que  $y_i^* > 0$  (no censurado) se puede expresar como:
        
$$E(y_i | x_i, y_i^* > 0) = \Phi(\alpha) \cdot x_i'\beta + \sigma \cdot \lambda(\alpha)$$


--- 

# Censura, Truncamiento y Modelos de Regresi贸n

## Truncamiento** por debajo ($L=0$):

- **Media Condicional:**  La media condicional de  $y_i$  dado  $x_i$  y que  $y_i^* \geq 0$  (no truncado) es:

$$E(y_i | x_i, y_i^* \geq 0) = x_i'\beta + E(\epsilon_i | x_i, \epsilon_i > 0).$$
          
- **Bajo normalidad:**  Asumiendo que  $\epsilon_i \sim N(0, \sigma^2)$, la media condicional se simplifica a: 
          
$$E(y_i | x_i, y_i^* \geq 0) = x_i'\beta + \sigma \cdot \lambda\left(\frac{x_i'\beta}{\sigma}\right).$$
      
--- 
      
# Estimador de Heckman (en dos pasos)
      
- El estimador de Heckman, tambi茅n conocido como el procedimiento de dos pasos de Heckman, se utiliza para corregir el sesgo de selecci贸n en modelos de regresi贸n con datos censurados o truncados.
    
- Este sesgo surge cuando la muestra observada no es representativa de la poblaci贸n, debido a la censura o el truncamiento.
    
## Primer paso:**
    
- **Estimaci贸n de la Probabilidad de Selecci贸n:**  Se estima un modelo Probit para la variable indicadora de selecci贸n  $d_i$, donde  $d_i = 1$  si  $y_i^* > 0$  (no censurado o no truncado) y  $d_i = 0$  en caso contrario.  El modelo Probit se estima utilizando la muestra completa.
- **C谩lculo de la Relaci贸n de Mills Inversa:**  Se utiliza la estimaci贸n del modelo Probit para calcular la relaci贸n de Mills inversa  $\hat{\lambda} = \frac{\phi(\hat{\alpha})}{\Phi(\hat{\alpha})}$, donde  $\hat{\alpha} = \frac{x_i'\hat{\beta}}{\hat{\sigma}}$  se obtiene a partir de las estimaciones del Probit.


--- 

# Estimador de Heckman (en dos pasos)
      
- El estimador de Heckman, tambi茅n conocido como el procedimiento de dos pasos de Heckman, se utiliza para corregir el sesgo de selecci贸n en modelos de regresi贸n con datos censurados o truncados.

- Este sesgo surge cuando la muestra observada no es representativa de la poblaci贸n, debido a la censura o el truncamiento.
    
## Segundo paso:

- **Regresi贸n con Correcci贸n de Sesgo:**  Se estima un modelo OLS de  $y_i$  en  $x_i$  y  $\hat{\lambda}(x_i'\hat{\beta})$, utilizando solo la muestra no censurada o no truncada  ($d_i = 1$).  La inclusi贸n de la relaci贸n de Mills inversa como regresor adicional corrige el sesgo de selecci贸n.

- **Intuici贸n:**  El primer paso estima la probabilidad de que una observaci贸n no est茅 censurada o truncada.  El segundo paso utiliza esta informaci贸n para ajustar la regresi贸n y corregir el sesgo.

- **Correcci贸n de Sesgo:**  El estimador de Heckman proporciona una forma de obtener estimaciones consistentes de los par谩metros del modelo en presencia de censura o truncamiento.

- **Matriz de Varianza:**  Es importante tener en cuenta que la matriz de varianza y los errores est谩ndar deben ajustarse para considerar la estimaci贸n en dos pasos.  Se utilizan m茅todos como el m茅todo de Murphy-Topel o el bootstrap para obtener errores est谩ndar consistentes.

---
  
# Truncamiento Incidental o Sesgo de Selecci贸n
  
## Introducci贸n
  
- **Relaci贸n con problemas de selectividad en la muestra:**

- Se conoce como **Sesgo de Selectividad Muestral** ( _Sample Selectivity Bias_ ).

- Ocurre cuando la muestra observada no es representativa de la poblaci贸n debido a un proceso de selecci贸n no aleatorio.

- **Ejemplo:** Consideremos el mercado laboral. Una persona **entra al mercado laboral** si el **salario ofrecido** es mayor al **costo de oportunidad** de trabajar. Por ende, una muestra del mercado laboral **excluye personas** para las cuales no es rentable entrar al mercado.

- **Implicaciones:**

- Esto genera un problema de **sesgo de selecci贸n**.
  
- **OLS es inconsistente** porque el salario observado est谩 correlacionado con la probabilidad de entrar al mercado laboral.

- **Contexto del mercado laboral**: Observamos  $y_i$  (salario) solo para las personas que trabajan  ($d_i = 1$). $d_i$  es una **variable dummy** que indica si una persona trabaja, 

$$ d_i = \begin{cases} 1 & \text{si trabaja} \\ 0 & \text{en caso contrario} \end{cases} $$
  
- **Variables Latentes**:
- $y_i^*$: Variable latente de resultado (salario, incluso para los que no trabajan).
- $d_i^*$: Variable latente que describe la decisi贸n de participar en el mercado laboral.

- **Definici贸n**:
  - Si  $d_i^* > 0$: La persona participa en el mercado laboral  ($d_i = 1$).
- Si  $d_i^* \leq 0$: La persona no participa en el mercado laboral  ($d_i = 0$).

---
# Truncamiento Incidental o Sesgo de Selecci贸n
  
## Modelo de Muestra de Selecci贸n. Modelo con Dos Ecuaciones
  
Para modelar el sesgo de selecci贸n, se utiliza un sistema de dos ecuaciones:
  
1. **Ecuaci贸n de Selecci贸n:**  Modela la decisi贸n de participar en el mercado laboral:
  
$$ d_i^* = z_i' \gamma + \epsilon_i, \quad d_i = 1 \text{ si } d_i^* > 0. $$

donde:
- $z_i$: Vector de variables que influyen en la decisi贸n de participar (e.g., educaci贸n, edad, estado civil).
- $\gamma$: Vector de coeficientes.
- $\epsilon_i$: T茅rmino de error.

2. **Ecuaci贸n de Resultado (_outcome_):**  Modela la variable de inter茅s (salario):

$$ y_i = x_i' \beta + u_i, \quad \text{observada solo si } d_i = 1. $$

donde:
- $x_i$: Vector de variables que influyen en el salario (e.g., experiencia, educaci贸n).
- $\beta$: Vector de coeficientes.
- $u_i$: T茅rmino de error.

**Ejemplo**: $d_i$ es Participaci贸n en el mercado laboral;  $y_i$ es Salario individual; $z_i$ son caracter铆sticas que afectan la participaci贸n (ej., nivel educativo); $x_i$ son caracter铆sticas que afectan el salario (e.g., experiencia laboral).

- **Selecci贸n No Aleatoria:** El modelo aborda la selecci贸n no aleatoria de la muestra, donde la decisi贸n de participar (o ser observado) est谩 correlacionada con la variable de resultado.
- **Correlaci贸n de Errores:**  Los errores  $\epsilon_i$  y  $u_i$  pueden estar correlacionados.  Esta correlaci贸n es la fuente del sesgo de selecci贸n, ya que implica que las variables no observadas que afectan la participaci贸n tambi茅n afectan el resultado.
- **Estimaci贸n Conjunta:**  Se requiere una estimaci贸n conjunta de las dos ecuaciones para obtener resultados consistentes.  El estimador de Heckman (de dos pasos) es un m茅todo com煤n para corregir el sesgo de selecci贸n en este tipo de modelos.


---
# Truncamiento Incidental o Sesgo de Selecci贸n
  
## Modelo Lineal
  
El truncamiento incidental o sesgo de selecci贸n surge cuando la observaci贸n de la variable dependiente est谩 condicionada a un proceso de selecci贸n.  Para modelar este tipo de datos, se utiliza un modelo con dos ecuaciones: una para la selecci贸n y otra para el resultado.

**Especificaci贸n:**  El modelo de truncamiento incidental se especifica mediante dos ecuaciones lineales:
  
1. **Ecuaci贸n de Participaci贸n:**
  
$$ y_i^{1*} = x_{i1}' \beta_1 + \epsilon_{i1} $$
  
Esta ecuaci贸n modela la decisi贸n binaria de participar o no en la muestra.  La variable latente  $y_i^{1*}$  representa la propensi贸n a participar, y  $x_{i1}$  son las variables que influyen en esta decisi贸n.

2. **Ecuaci贸n de Outcome:**

$$ y_i^{2*} = x_{i2}' \beta_2 + \epsilon_{i2} $$
  
Esta ecuaci贸n modela la variable de resultado  $y_i^{2*}$, que solo se observa si el individuo decide participar  ($y_i^{1*} > 0$).  Las variables  $x_{i2}$  son las que influyen en el resultado.

**Relaci贸n con el Modelo Tobit:**  El modelo Tobit es un caso particular de este modelo donde  $y_i^{1*} = y_i^{2*}$, es decir, la variable latente que determina la selecci贸n es la misma que la variable de resultado.


**Supuesto de Normalidad:**  Se asume que los errores de ambas ecuaciones,  $\epsilon_{i1}$  y  $\epsilon_{i2}$, siguen una distribuci贸n normal bivariada:
  
$$\begin{pmatrix}
\epsilon_{i1} \\
\epsilon_{i2}
\end{pmatrix} \sim \mathcal{N}
\begin{pmatrix}
0, &
\begin{pmatrix}
\sigma_1^2 & \rho\sigma_1\sigma_2 \\
\rho\sigma_1\sigma_2 & \sigma_2^2
\end{pmatrix}
\end{pmatrix}.$$
  
**Implicaciones:**
  
- $\rho$:  El par谩metro  $\rho$  representa la correlaci贸n entre los errores de la ecuaci贸n de participaci贸n y la ecuaci贸n de resultado.
- **Consistencia:**  Si  $\rho \neq 0$, existe una correlaci贸n entre las variables no observadas que afectan la participaci贸n y las que afectan el resultado.  Ignorar esta correlaci贸n al estimar la ecuaci贸n de resultado resultar谩 en estimaciones sesgadas e inconsistentes.

### **Construcci贸n de la Verosimilitud**  

Para construir la funci贸n de verosimilitud, necesitamos considerar dos casos:
  
1. **Participaci贸n:**  Si  $y_i^{1*} > 0$, observamos  $y_i^{2*}$.  La contribuci贸n a la verosimilitud en este caso es la probabilidad conjunta de observar  $y_i^{1*} > 0$  y  $y_i^{2*}$:

$$ P(y_i^{1*} > 0) \cdot f(y_i^{2*} | y_i^{1*} > 0). $$
  
2. **No Participaci贸n:**  Si  $y_i^{1*} \leq 0$, no observamos  $y_i^{2*}$.  La contribuci贸n a la verosimilitud en este caso es simplemente la probabilidad de observar  $y_i^{1*} \leq 0$:

$$ P(y_i^{1*} \leq 0). $$
  
La log-verosimilitud para la muestra completa se obtiene sumando las contribuciones de cada observaci贸n, teniendo en cuenta si participa o no en la muestra.

**Observaciones:**
  
- **Correlaci贸n entre las Ecuaciones:**  La correlaci贸n  $\rho$  entre los errores de las dos ecuaciones captura la dependencia entre la decisi贸n de participar y el resultado.  Si  $\rho$  es diferente de cero, es crucial tener en cuenta esta correlaci贸n en la estimaci贸n.

- **Modelo de Selecci贸n:**  Este modelo proporciona un marco para incorporar expl铆citamente la selectividad muestral en el an谩lisis de regresi贸n, permitiendo obtener estimaciones consistentes de los par谩metros de inter茅s.


**Funci贸n de Verosimilitud para el Modelo de Sesgo de Selecci贸n**
  
La funci贸n de verosimilitud puede escribirse de la siguiente manera:
  
$$\mathcal{L} = \prod_{i=1}^{N} \left[ P(y_i^{1*} \leq 0) \right]^{1-d_i} \cdot \left[ f(y_i^{2*} | y_i^{1*} > 0) \cdot P(y_i^{1*} > 0) \right]^{d_i}$$
donde $d_i$ es una variable indicadora que toma el valor de 1 si el individuo participa ($y_i^{1*} > 0$) y 0 en caso contrario.

La funci贸n de verosimilitud es el producto de las probabilidades de observar cada individuo en la muestra.  Para los individuos que no participan  ($d_i = 0$), la probabilidad es simplemente  $P(y_i^{1*} \leq 0)$.  Para los individuos que participan  ($d_i = 1$), la probabilidad es la probabilidad conjunta de observar  $y_i^{1*} > 0$  y  $y_i^{2*}$.

---
# Truncamiento Incidental o Sesgo de Selecci贸n
  
## Supuesto de Normalidad
  
**Elemento Clave** para simplificar la funci贸n de verosimilitud, se asume que los errores  $\epsilon_{i1}$  y  $\epsilon_{i2}$  tienen una distribuci贸n conjunta bivariada normal est谩ndar:
  
$$\begin{pmatrix}
\epsilon_{i1} \\
\epsilon_{i2}
\end{pmatrix} \sim \mathcal{N}
\begin{pmatrix}
0, &
  \begin{pmatrix}
1 & \rho \\
\rho & 1
\end{pmatrix}
\end{pmatrix}.$$
  
**Probabilidad de Participaci贸n:** Bajo este supuesto, la probabilidad de participaci贸n se puede expresar como:
  
$$P(y_i^{1*} > 0) = \Phi\left( \frac{x_{i1}'\beta_1}{\sqrt{1-\rho^2}} \right)$$

donde  $\Phi(\cdot)$  es la funci贸n de distribuci贸n acumulada de la normal est谩ndar.


---

# Truncamiento Incidental o Sesgo de Selecci贸n

## Supuesto de Normalidad

Bajo el supuesto de normalidad, la log-verosimilitud para este modelo est谩 dada por:

$$\ell(\beta_1, \beta_2, \rho) = \sum_{y_i = 0} \ln\left( \Phi\left( \frac{x_{i1}'\beta_1}{\sqrt{1-\rho^2}} \right) \right)
+ \sum_{y_i = 1} \ln\left( \phi\left( \frac{y_i^{2*} - x_{i2}'\beta_2}{\sqrt{1-\rho^2}} \right) \cdot \Phi\left( \frac{x_{i1}'\beta_1 + \rho \frac{y_i^{2*} - x_{i2}'\beta_2}{\sqrt{1-\rho^2}}}{\sqrt{1-\rho^2}} \right) \right).$$

donde  $\phi(\cdot)$  es la funci贸n de densidad de la normal est谩ndar.

---
# Truncamiento Incidental o Sesgo de Selecci贸n

## M茅todo de Heckman en Dos Pasos (Heckit)

**Alternativa a la M谩xima Verosimilitud**  El m茅todo de Heckman en dos pasos es una alternativa a la estimaci贸n por m谩xima verosimilitud.  Es computacionalmente menos costoso, pero requiere el supuesto de normalidad de los errores.

Idea general de los pasos:

1.  **Relaci贸n entre los Errores:**  Bajo el supuesto de normalidad bivariada, la relaci贸n entre los errores de las dos ecuaciones se puede escribir como:

$$\epsilon_{i2} = \rho \epsilon_{i1} + \xi_i,$$

donde  $\xi_i$  es independiente de  $\epsilon_{i1}$  y  $\xi_i \sim \mathcal{N}(0, \sigma^2_\xi)$.

2. **Media Condicional:**  La media condicional del outcome  ($y_i^{2*}$)  dado que el individuo participa  ($y_i^{1*} > 0$)  es:

$$\mathbb{E}(y_i^{2*} | y_i^{1*} > 0) = x_{i2}'\beta_2 + \rho \lambda(x_{i1}'\beta_1),$$

donde  $\lambda(\cdot)$  es el **Inverse Mills Ratio** $\lambda(x) = \frac{\phi(x)}{\Phi(x)}$.


---
# Truncamiento Incidental o Sesgo de Selecci贸n

## M茅todo de Heckman en Dos Pasos (Heckit)

**Correcci贸n del Sesgo:**  El t茅rmino  $\rho \lambda(x_{i1}'\beta_1)$  corrige el sesgo de selecci贸n.  La relaci贸n de Mills inversa  ($\lambda(x)$)  captura la informaci贸n sobre la selecci贸n no aleatoria de la muestra.
  
  El m茅todo de Heckman, tambi茅n conocido como Heckit, es un procedimiento en dos pasos que se utiliza para corregir el sesgo de selecci贸n en modelos de regresi贸n.  Este m茅todo es una alternativa a la estimaci贸n por m谩xima verosimilitud completa y es especialmente 煤til cuando se asume una distribuci贸n normal bivariada para los errores.
  
  
---
# Modelo Heckit
    
## Pasos:
    
1. **Probit para la Participaci贸n:**
    
- Se estima un modelo probit para la variable indicadora de participaci贸n  $d_i$, donde  $d_i = 1$  si el individuo participa  ($y_i^{1*} > 0$)  y  $d_i = 0$  en caso contrario. 
  
- La probabilidad de participaci贸n se modela como:
    
$$P(d_i = 1 | x_{i1}) = \Phi(x_{i1}'\beta_1),$$

donde  $\Phi(\cdot)$  es la funci贸n de distribuci贸n acumulada de la normal est谩ndar.

- Se obtienen las estimaciones de los coeficientes  $\hat{\beta}_1$  del modelo probit.

- Se calcula la relaci贸n de Mills inversa  ($\lambda$)  para cada individuo utilizando las estimaciones del probit:

$$\hat{\lambda}_i = \lambda(x_{i1}'\hat{\beta}_1) = \frac{\phi(x_{i1}'\hat{\beta}_1)}{\Phi(x_{i1}'\hat{\beta}_1)},$$
    
---
# Modelo Heckit
    
2. **Regresi贸n OLS Aumentada**
    
- Se estima la ecuaci贸n de resultado  ($y_i^{2*}$)  mediante una regresi贸n OLS que incluye la relaci贸n de Mills inversa  ($\hat{\lambda}_i$)  como una variable explicativa adicional:
    
$$y_i^{2*} = x_{i2}'\beta_2 + \rho \sigma_2 \hat{\lambda}_i + \nu_i,$$
  
donde  $\rho$  es la correlaci贸n entre los errores de las dos ecuaciones y  $\sigma_2$  es la desviaci贸n est谩ndar del error en la ecuaci贸n de resultado.

- La inclusi贸n de  $\hat{\lambda}_i$  corrige el sesgo de selecci贸n.

---
# Modelo Heckit

## Ventajas:

- **Simplicidad:**  Es f谩cil de implementar, ya que solo requiere la estimaci贸n de un modelo probit y una regresi贸n OLS.
- **Amplitud:**  Es aplicable a una amplia variedad de modelos de selecci贸n, como el an谩lisis del mercado laboral, la participaci贸n en programas sociales y las decisiones de inversi贸n.
- **Supuestos:**  Requiere menos supuestos que la m谩xima verosimilitud completa, pero asume la normalidad conjunta de los errores  $\epsilon_{i1}$  y  $\epsilon_{i2}$.

---
# Modelo Heckit

## Prueba de Hip贸tesis:

- Se puede realizar una prueba de hip贸tesis para determinar si la correlaci贸n entre los errores  ($\rho$)  es significativamente diferente de cero: $H_0: \rho = 0$.
- Si se rechaza la hip贸tesis nula  ($H_0$), se recomienda utilizar la m谩xima verosimilitud completa (MLE) en lugar de OLS, ya que OLS no captura adecuadamente el sesgo de selecci贸n cuando  $\rho \neq 0$.

---
# Modelo Heckit

## Comparaci贸n de M茅todos:

- Es recomendable comparar los resultados del m茅todo de Heckman (Heckit) con los de la m谩xima verosimilitud completa (MLE) en un ejemplo pr谩ctico.  Esto ayuda a comprender las diferencias entre los m茅todos y el impacto del sesgo de selecci贸n en las estimaciones.
- La comparaci贸n de los m茅todos tambi茅n puede ayudar a clarificar las diferencias entre selecci贸n, censura y truncamiento, y a comprender c贸mo cada uno de estos problemas afecta la estimaci贸n de los modelos de regresi贸n.

**Nota:**  Siempre es importante entender los supuestos y limitaciones de cada m茅todo para seleccionar el m谩s adecuado en contextos pr谩cticos.


---

layout: false

class: inverse, center, middle

# 隆Muchas gracias!

<span style="color:#f59f18; font-size:1.3em; font-weight:bold;">驴Preguntas?</span>

<div style="margin-top: 50px;"></div>


**Felipe J. Quezada-Escalona**  

<img src="https://fquezadae.github.io/Slides-Econometria/figs/depto_economia_blanco.png" width="250">

<a href="https://felipequezada.com" target="_blank" style="
  font-size:1em;
  background:linear-gradient(#f59f18);
  -webkit-background-clip:text;
  -webkit-text-fill-color:transparent;
  text-decoration:none;
">
 felipequezada.com
</a>

```{r message=FALSE, warning=FALSE, include=FALSE}
pagedown::chrome_print("11-Modelo-Datos-Truncados.html", output = "11-Modelo-Datos-Truncados.pdf")
```
